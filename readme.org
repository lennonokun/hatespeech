 config

active tasks + datasets --> dataset/dataloader

* notes

- targets perform much much better in 8b997 for some reason, but only with
  adapterplus???
- in 8b997 lora not working at all???
- checkout after 8b997 but with adapterplus?
- compare masked-gnu-865 to exultant-skink-962
- not head shape
- significant difference when merging partitioned targets
  (0.52 vs 0.43 macro f1)

** limitations
- AutoAdapterModel cannot do quantized LoRA
- adapters.init(AutoModel) cannot do custom heads

** quantization
- do not use Trainer(precision=...) ???
- peft has better support, but much less flexible otherwise
  - lora vars work as fp32
- adapters has worse support
  - lora vars need to match AutoModel's torch_dtype
  - can fix by manually changing, seems to maybe minorly affect metrics?
- currently heads are still float32, but probably worth it for stability

** compilation
- maybe worth it???
- ~10% speed increase
- yet to try on base-discriminator

** model size
- larger model greatly improves target f1
- comparable label f1 

** hydra
- possible only with train_adapter(list) which gives deprecation warning

** isolation
- todo

** else
- rationale seems to get stuck at 0.6 f1?
- warm up heads esp when fusing/merging
- probably heads should have their own learning rate?
- more params necessary for target
- fully separate targets? 
- why is the adapter fusion 2.4M params

* todo

** ideas
- TASKS SHOULD MOSTLY JUST BE DATASET + NAME, PUT REST IN HEAD?
- restructure like lightning-hydra-zen-template
- standardize TaskSet vs Heads (set or plural)
- add PreTrainedModel to custom types?
- separate heads loss + metrics ? (loss could go to mtlloss?)
- re-add task-combined dataset for mtllora? (but it performed much more poorly)
- make sure WeightedSampling good
- make HateDatasets something before HateDataModule?
- just one paths/info config instead of stats + datasetinfo

** short-term
- saving + loading also for full?
- allow for multiple adapters to be used at same time?
- improve modeling/methods.py:adjust_dtypes (classvar for each method?)
- partition targets with different models and merge?
- smaller partitions?
- re-add target-specific metrics
- target-specific loss currently just index of mask, not corresponding
  to actual targets
- eval during validation / test?
- actually save adapters
- re-add VAT + gradnorm, check if worth
- fixed step epochs with equal sampling from datasets?
- task batches should include all dataset labels
- separate data loading into its own module
- update readme papers + datasets

** medium-term
- try more virtual adversarial training
- try label smoothing?
- attention mechanism with rationales?
- rationales mask including not classifying invalid tokens

** long term
- text augmentation
- use hatebase for lexicon features

* credits
- MultiLR sourced from https://github.com/kardasbart/MultiLR

* papers
  - https://bit.kuas.edu.tw/~jni/2024/vol9/s1/36.JNI-S-2023-08-006.pdf
  - https://arxiv.org/pdf/1806.08028
  
* datasets

** using
- hatexplain
  - pub: https://arxiv.org/pdf/2012.10289.pdf
  - data: https://github.com/punyajoy/HateXplain
  - proportion abusive: 0.57
- measuring hate speech
  - data: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech

** considering
- large scale crowdsourcing + characterization...
  - pub: https://arxiv.org/pdf/1802.00393.pdf
  - data: https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN
  - requested data but it says to follow email but i dont see yet
- hateval semeval-2019 task 5?
  - pub: https://www.aclweb.org/anthology/S19-2007
  - proportion abusive: 0.4
  - specific to women and immigrants
  - data link not working?
- ethos: an online hate speech detection dataset (binary)
  - pub: https://arxiv.org/pdf/2006.08328.pdf
  - proportion abusive: 0.33
- twitter sentiment analysis
  - data:
    https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech
  - proportion abusive: 0.07
  - racism/sexism specific

* results

** initial n=10 norm (fixed) + RW + 20 epochs + r=8 + layers=2

| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.610 |
| test_rationale_f1 |   0.659 |
| test_score_mse    |   1.936 |
| test_target_f1    |   0.721 |

** small

*** bn

**** f=64
| metrics        | Single Task | Single Merge |
|----------------+-------------+--------------|
| test_label_f1  |       0.608 |        0.637 |
| test_score_mse |       1.837 |              |

*** lora

**** r=16 a=16

| metrics        | Single Task | Single Merge |
|----------------+-------------+--------------|
| test_label_f1  |       0.644 |        0.637 |
| test_score_mse |       1.769 |              |

** base

*** bn

**** f=64
| metrics        | Single Task | Single Merge |
|----------------+-------------+--------------|
| test_label_f1  |       0.664 |        0.647 |
| test_score_mse |       1.457 |              |

*** lora

**** r=8

| metrics        | Single Task | Single Merge |
|----------------+-------------+--------------|
| test_label_f1  |       0.658 |        0.657 |
| test_score_mse |       1.439 |              |
| test_target_f1 |       0.641 |              |

