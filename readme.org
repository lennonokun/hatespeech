* about

This project hopes to test and compare multi-task learning and modular
learning methods on the problem of hatespeech
classification. Currently the datasets of interest are the "HateXplain"
and "Measuring Hate Speech" datasets. I specifically hope to compare
merging/fusing separately trained adapters with singly trained
multi-task adapters.

* how to use

** importing the data

The datasets themselves are not included in this repository. Below are
instructions on how to import them manually (I will probably add
scripts for this soon).

HateXplain:
- download [[https://github.com/hate-alert/HateXplain/blob/master/Data/dataset.json][dataset.json]] from their github repository and move it to
  data/explain/dirty.json

Measuring Hate Speech:
- download [[https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech/blob/main/measuring-hate-speech.parquet][measuring-hate-speech.parquet]] from their hugging face
  dataset and move it to data/measuring/input.parquet

** preprocessing the data

After importing the datasets you wish to use, use the included scripts
to preprocess them:

HateXplain:
- first, convert the json to parquet with ~uv run -m
  scripts.fix_explain~
- then, preprocess with ~uv run -m scripts.preprocess preprocessor=explain~
  
Measuring Hate Speech:
- preprocess with ~uv run -m scripts.preprocessor
  preprocessor=measuring~

** training and testing models

Pretrained models are included in the adapters folder. Currently, they
are not all in the proper format except for those in
adapters/small/lora16. I will write a thorough explanation soon,
but for now check out how to train and test the models by looking at
the code and the examples in the experiments section.

* experiments

** electra-small lora16 targets partitioned n=4

*** training
#+begin_src sh
# train partitions
uv run -m scripts.model tasks=[target] tasks.target.mask_pattern="1 4" \
	 save_path=adapters/small/lora16/target_part_1
uv run -m scripts.model tasks=[target] tasks.target.mask_pattern="2 4" \
	 save_path=adapters/small/lora16/target_part_2
uv run -m scripts.model tasks=[target] tasks.target.mask_pattern="3 4" \
	 save_path=adapters/small/lora16/target_part_3
uv run -m scripts.model tasks=[target] tasks.target.mask_pattern="4 4" \
	 save_path=adapters/small/lora16/target_part_4
# train merge
uv run -m scripts.model tasks=[target] method=ah_merge16 optimization.warmup=15 \
	 method.sources="'adapters/small/lora16/target_part_*'" \
	 save_path=adapters/small/lora16/target_merge 
# train single
uv run -m scripts.model tasks=[target] save_path=adapters/small/lora16/target_single
#+end_src

*** testing
#+begin_src sh
# test partitions
uv run -m scripts.model tasks=[target] action=test tasks.target.mask_pattern="1 4" \
	 method=ah_load heads.load=true load_path=adapters/small/lora16/target_part_1
uv run -m scripts.model tasks=[target] action=test tasks.target.mask_pattern="2 4" \
	 method=ah_load heads.load=true load_path=adapters/small/lora16/target_part_2
uv run -m scripts.model tasks=[target] action=test tasks.target.mask_pattern="3 4" \
	 method=ah_load heads.load=true load_path=adapters/small/lora16/target_part_3
uv run -m scripts.model tasks=[target] action=test tasks.target.mask_pattern="4 4" \
	 method=ah_load heads.load=true load_path=adapters/small/lora16/target_part_4
uv run -m scripts.partition_f1 results_glob="'adapters/small/lora16/target_part_*/results.csv'"
# test merge
uv run -m scripts.model tasks=[target] action=test method=ah_load heads.load=true \
	 load_path=adapters/small/lora16/target_merge
# train single
uv run -m scripts.model tasks=[target] action=test method=ah_load heads.load=True \
	load_path=adapters/small/lora16/target_single
#+end_src

*** results
- it seems as if merged model is comparable to multiple subtasks
  models and better than a singly trained model
- make sure this isnt a result of too early stopping

| metrics                   | partitioned | single | merged (warmup=15) |
|---------------------------+-------------+--------+--------------------|
| test_target_f1_african    |       0.790 |  0.780 |              0.814 |
| test_target_f1_arab       |       0.630 |  0.494 |              0.521 |
| test_target_f1_asian      |       0.653 |  0.651 |              0.667 |
| test_target_f1_caucasian  |       0.405 |  0.322 |              0.395 |
| test_target_f1_hispanic   |       0.725 |  0.540 |              0.605 |
| test_target_f1_homosexual |       0.806 |  0.761 |              0.784 |
| test_target_f1_islam      |       0.721 |  0.673 |              0.694 |
| test_target_f1_jewish     |       0.889 |  0.877 |              0.880 |
| test_target_f1_other      |       0.281 |  0.247 |              0.270 |
| test_target_f1_refugee    |       0.648 |  0.570 |              0.624 |
| test_target_f1_women      |       0.572 |  0.524 |              0.580 |
| test_target_macro_f1      |       0.647 |  0.585 |              0.621 |
| test_target_micro_f1      |       0.680 |  0.609 |              0.666 |

* notes
- could pre train adapter on much much larger non labeled corpus
  (language adapters)
- there seems to be evidence that at least merging partitioned targets
  improves metrics
- todo try longer encoder warmups, larger models, fusion, multi
  dataset, other tasks
- todo parallel, but probably they should not be trainable
- todo preprocessing stats file should be per dataset
- todo better logging
- should preprocessing have dataset be an argument and not
  preprocessor be a store
- todo use more targets?

** limitations
- AutoAdapterModel cannot do quantized LoRA TODO check
- adapters.init(AutoModel) cannot do custom heads

** quantization
- do not use Trainer(precision=...) ???
- peft has better support, but much less flexible otherwise
  - lora vars work as fp32
- adapters has worse support
  - lora vars need to match AutoModel's torch_dtype
  - can fix by manually changing, seems to maybe minorly affect metrics?
- currently heads are still float32, but probably worth it for stability

** compilation
- maybe worth it???
- ~10% speed increase
- yet to try on base-discriminator

** model size
- larger model greatly improves target f1
- comparable label f1 

** else
- rationale seems to get stuck at 0.6 f1?
- warm up heads esp when fusing/merging
- probably heads should have their own learning rate?
- more params necessary for target
- fully separate targets? 
- why is the adapter fusion 2.4M params

* todo

** ideas
- TASKS SHOULD MOSTLY JUST BE DATASET + NAME, PUT REST IN HEAD?
- restructure like lightning-hydra-zen-template
- add PreTrainedModel to custom types?
- separate heads loss + metrics ? (loss could go to mtlloss?)
- re-add task-combined dataset for mtllora? (but it performed much more poorly)
- make sure WeightedSampling good
- make HateDatasets something before HateDataModule?
- just one paths/info config instead of stats + datasetinfo

** short-term
- saving + loading also for full?
- allow for multiple adapters to be used at same time?
- improve modeling/methods.py:adjust_dtypes (classvar for each method?)
- smaller partitions?
- re-add head warmup for merging
- re-add target-specific metrics
- target-specific loss currently just index of mask, not corresponding
  to actual targets
- eval during validation / test?
- re-add VAT + gradnorm, check if worth
- fixed step epochs with equal sampling from datasets?
- separate data loading into its own module
- update readme papers + datasets

** medium-term
- try more virtual adversarial training
- try label smoothing?
- attention mechanism with rationales?
- rationales mask including not classifying invalid tokens

** long term
- text augmentation
- use hatebase for lexicon features

* credits
- MultiLR sourced from https://github.com/kardasbart/MultiLR

* papers
  - https://bit.kuas.edu.tw/~jni/2024/vol9/s1/36.JNI-S-2023-08-006.pdf
  - https://arxiv.org/pdf/1806.08028
  
* datasets

** using
- hatexplain
  - pub: https://arxiv.org/pdf/2012.10289.pdf
  - data: https://github.com/punyajoy/HateXplain
  - proportion abusive: 0.57
- measuring hate speech
  - data: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech

** considering
- large scale crowdsourcing + characterization...
  - pub: https://arxiv.org/pdf/1802.00393.pdf
  - data: https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN
  - requested data but it says to follow email but i dont see yet
- hateval semeval-2019 task 5?
  - pub: https://www.aclweb.org/anthology/S19-2007
  - proportion abusive: 0.4
  - specific to women and immigrants
  - data link not working?
- ethos: an online hate speech detection dataset (binary)
  - pub: https://arxiv.org/pdf/2006.08328.pdf
  - proportion abusive: 0.33
- twitter sentiment analysis
  - data:
    https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech
  - proportion abusive: 0.07
  - racism/sexism specific

