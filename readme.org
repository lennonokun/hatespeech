* notes
- layernorm seems to help a lot
- lora dropout maybe also
- rationale seems to get stuck at 0.6 f1?
- probably heads should have their own learning rate?
- check mtl_importances working
- check if can do branching adapters with LoraConfig leave_out
- reconsider quantization

* todo

** short-term
- re-add VAT
- WEIGH TASKS SEPARATELY BY TOTAL FREQUENCY
- task batches should include all dataset labels
- separate data loading into its own module
- update readme papers + datasets

** medium-term
- branching adapters
- try more virtual adversarial training
- try label smoothing?
- attention mechanism with rationales?
- rationales mask including not classifying invalid tokens

** long term
- text augmentation
- use hatebase for lexicon features

* papers
  - https://bit.kuas.edu.tw/~jni/2024/vol9/s1/36.JNI-S-2023-08-006.pdf
  - https://arxiv.org/pdf/1806.08028
  
* datasets

** using
- hatexplain
  - pub: https://arxiv.org/pdf/2012.10289.pdf
  - data: https://github.com/punyajoy/HateXplain
  - proportion abusive: 0.57
- measuring hate speech
  - data: https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech

** considering
- large scale crowdsourcing + characterization...
  - pub: https://arxiv.org/pdf/1802.00393.pdf
  - data: https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN
  - requested data but it says to follow email but i dont see yet
- hateval semeval-2019 task 5?
  - pub: https://www.aclweb.org/anthology/S19-2007
  - proportion abusive: 0.4
  - specific to women and immigrants
  - data link not working?
- ethos: an online hate speech detection dataset (binary)
  - pub: https://arxiv.org/pdf/2006.08328.pdf
  - proportion abusive: 0.33
- twitter sentiment analysis
  - data:
    https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech
  - proportion abusive: 0.07
  - racism/sexism specific

* results

** rolling n=10 normalization + uncertainty weighing + gradient adversarial training

| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.559 |
| test_rationale_f1 |   0.710 |
| test_target_f1    |   0.688 |

** initial n=10 normalization + uncertainty weighing + gradient adversarial training
| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.544 |
| test_rationale_f1 |   0.609 |
| test_target_f1    |   0.669 |

** initial n=10 norm (broken) + DWA (T=2) + GAT (eps=0.6)
| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.541 |
| test_rationale_f1 |   0.671 |
| test_target_f1    |   0.639 |

** initial n=10 norm (broken) + DWA (T=2) + 30 epochs
| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.572 |
| test_rationale_f1 |   0.798 |
| test_target_f1    |   0.705 |

** initial n=10 norm (fixed) + RW + 10 epochs + r=8 + layers=2
| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.592 |
| test_rationale_f1 |   0.604 |
| test_score_mse    |   2.031 |
| test_target_f1    |   0.712 |

** initial n=10 norm (fixed) + RW + 20 epochs + r=8 + layers=2

| metrics           | results |
|-------------------+---------|
| test_label_f1     |   0.610 |
| test_rationale_f1 |   0.659 |
| test_score_mse    |   1.936 |
| test_target_f1    |   0.721 |
